{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sample for FID evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, tqdm, os, shutil, numpy as np\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from modules.util import instantiate_from_config\n",
    "from models.diffusion.ddim import DDIMSampler\n",
    "from models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "\n",
    "ROOT_PATH = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting\"\n",
    "\n",
    "dataset_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart\"\n",
    "dataset_subdivided_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_subdivided_256\"\n",
    "mapping_file = \"/mnt/data1/bardella_data/gitRepos/Thesis/dataset_preprocessing/wikiart/mapping.json\"\n",
    "\n",
    "\n",
    "# Load the label literal-discrete mapping\n",
    "with open(mapping_file, \"r\") as fin:\n",
    "    mapping = json.load(fin)\n",
    "    mapping_r = {v:k for k, v in mapping}\n",
    "\n",
    "# Load the dataset image path and labels\n",
    "with open(dataset_dir+\"/dataset.json\", \"r\") as fin:\n",
    "    dataset_info = json.load(fin)[\"labels\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset per color subdivision and metadata generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Subdivision per color label\n",
    "# new_dataset_info = {\"labels\":[]}\n",
    "\n",
    "# if not os.path.exists(dataset_subdivided_dir):\n",
    "#     for data in tqdm.tqdm(dataset_info):\n",
    "#         img_dir, label = data\n",
    "\n",
    "#         img_name, mapped_label = img_dir.split(\"/\")[1], mapping_r[label]\n",
    "\n",
    "#         dest_dir = \"/\".join((dataset_subdivided_dir,f\"{mapped_label}\"))\n",
    "\n",
    "#         new_dataset_info[\"labels\"].append([\"/\".join((mapped_label, img_name)), label])\n",
    "\n",
    "#         if not os.path.exists(dest_dir):\n",
    "#             os.makedirs(dest_dir)\n",
    "#         shutil.copy(dataset_dir + f\"/{img_dir}\", dest_dir)\n",
    "        \n",
    "\n",
    "#     with open(dataset_subdivided_dir+\"/dataset.json\", \"w\") as fout:\n",
    "#         json.dump(new_dataset_info, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from albumentations.augmentations.geometric.resize import Resize\n",
    "\n",
    "if not os.path.exists(dataset_subdivided_dir):\n",
    "    new_dataset_info = {\"labels\":[]}\n",
    "\n",
    "    src_dir = dataset_dir\n",
    "    dest_dir = dataset_subdivided_dir\n",
    "    resize_transform = Resize(height=256, width=256,always_apply=True)\n",
    "\n",
    "    for image_path, numerical_label in tqdm(dataset_info[:None]):\n",
    "\n",
    "        image_name, letteral_label = image_path.split(\"/\")[-1], mapping_r[numerical_label]\n",
    "\n",
    "        image = cv2.imread(src_dir + f\"/{image_path}\")\n",
    "        image_cvt = resize_transform(image=image)[\"image\"]\n",
    "\n",
    "        new_dataset_info[\"labels\"].append([\"/\".join((letteral_label, image_name)), numerical_label])\n",
    "\n",
    "        if not os.path.exists(dest_dir + f\"/{letteral_label}\"):\n",
    "            os.makedirs(dest_dir + f\"/{letteral_label}\")\n",
    "        \n",
    "        cv2.imwrite(dest_dir + f\"/{letteral_label}/{image_name}\", image_cvt)\n",
    "\n",
    "    with open(dest_dir+\"/dataset.json\", \"w\") as fout:\n",
    "        json.dump(new_dataset_info, fout)\n",
    "else:\n",
    "    print(\"This subdivision already exist\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Samples from the LD model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained LDM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "config = OmegaConf.load(ROOT_PATH + \"/configs/custom-ldm-cwa-vq-f8.yaml\")  \n",
    "sample_folder = ROOT_PATH + f\"/sample/ldm/wikiart\"\n",
    "\n",
    "vq_gan_pretrained_ckpt_path = ROOT_PATH + \"/pretrained_model/vq-f8/model.ckpt\"\n",
    "ldm_pretrained_ckpt_path = ROOT_PATH + \"/model_checkpts/ldm/wikiart/epoch=299-step=69600.ckpt\"\n",
    "config.model.params.first_stage_config.params[\"ckpt_path\"] = vq_gan_pretrained_ckpt_path\n",
    "\n",
    "model = load_model_from_config(config, ldm_pretrained_ckpt_path)\n",
    "# sampler = DDIMSampler(model)\n",
    "# sampler_type = \"DDIM\"\n",
    "sampler = PLMSSampler(model)\n",
    "sampler_type = \"PLMS\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new Sample for a specific class. \n",
    "This operation is very time consuming and require a lot of memory.\n",
    "\n",
    "The problem is we can only take about 15 samples at a time and it takes a few minuts for each batch.\n",
    "\n",
    "To sample 10_000 with 200 denoising timesteps we need 11 hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy\n",
    "\n",
    "def computeFidPytorch(truth_dataset, generated_dataset, device):\n",
    "\n",
    "    output = subprocess.run(['python', \n",
    "                            '-m', \n",
    "                            'pytorch_fid', \n",
    "                            truth_dataset, \n",
    "                            generated_dataset, \n",
    "                            \"--device\", \n",
    "                            device], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    \n",
    "\n",
    "    return numpy.float32(output.split(\" \")[-1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = [0,1,2,3,4,5,6,7,8]   # define classes to be sampled here\n",
    "classes = [1, 2]   # Choose the classes that we want the model sample from\n",
    "\n",
    "# As said by the official TF FID repo we should generate at least 10000 sample \n",
    "# in order to have a FID which is not sub-optimal \n",
    "\n",
    "n_samples_per_class = 10_000 \n",
    "\n",
    "max_batch_size = 15\n",
    "FIDs = {}\n",
    "if os.path.exists(ROOT_PATH + \"/FIDs.npy\"):\n",
    "    with open(ROOT_PATH + \"/FIDs.npy\", \"rb\") as fin:\n",
    "        FIDs = np.load(fin, allow_pickle=True).item()\n",
    "\n",
    "whenFID = [1000 * i for i in range(1, 11)]\n",
    "\n",
    "unconditional_class = 8\n",
    "\n",
    "# ddim_steps = [20, 50, 200, 500, 1000]\n",
    "ddim_steps = [75]\n",
    "ddim_eta = 0\n",
    "scale = 2  # for unconditional guidance\n",
    "\n",
    "for ddim_step in ddim_steps:\n",
    "    for cls in classes:\n",
    "\n",
    "        # Create the folder where to save the sample\n",
    "        sample_already_created = 0\n",
    "        save_folder = sample_folder + f\"/{sampler_type}/{ddim_step}/{mapping_r[cls]}\"\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        else:\n",
    "            sample_already_created = len([file for file in os.listdir(save_folder) if os.path.isfile(os.path.join(save_folder, file))])\n",
    "\n",
    "        image_counter = sample_already_created\n",
    "        with torch.no_grad():\n",
    "            with model.ema_scope():\n",
    "                \n",
    "                n_samples_per_class -= sample_already_created\n",
    "                print(f\"\\nRendering {n_samples_per_class} examples of class '{mapping_r[cls]}' in {ddim_step} steps and using s={scale:.2f}.\")\n",
    "                if n_samples_per_class > max_batch_size:\n",
    "                    sub_batches = [max_batch_size] * (n_samples_per_class//max_batch_size)\n",
    "                    \n",
    "                    if n_samples_per_class % max_batch_size != 0 :\n",
    "                        sub_batches.append(n_samples_per_class % max_batch_size)\n",
    "\n",
    "                else:\n",
    "                    sub_batches = [n_samples_per_class]\n",
    "\n",
    "                for idx_sub_batch, sub_batch_size in enumerate(sub_batches):\n",
    "                    # Un-conditional class for free classifier sampling\n",
    "                    uc = model.get_learned_conditioning({model.cond_stage_key: torch.tensor(sub_batch_size*[unconditional_class]).to(model.device)}) if scale != 1 else None\n",
    "                    \n",
    "                    #Conditional class\n",
    "                    xc = torch.tensor(sub_batch_size*[cls])\n",
    "                    c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
    "                    \n",
    "                    samples_ddim, _ = sampler.sample(S=ddim_step,\n",
    "                                                    conditioning=c,\n",
    "                                                    batch_size=sub_batch_size,\n",
    "                                                    shape=[4, 32, 32],\n",
    "                                                    verbose=False,\n",
    "                                                    unconditional_guidance_scale=scale,\n",
    "                                                    unconditional_conditioning=uc, \n",
    "                                                    eta=ddim_eta)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0).cpu()\n",
    "\n",
    "                    # Save the just generated images and delete\n",
    "                    for idx_img, images in enumerate(torch.chunk(x_samples_ddim, chunks = sub_batch_size, dim = 0)):\n",
    "                        \n",
    "                        final_image = 255. * rearrange(torch.squeeze(images, dim=0), 'c h w -> h w c').numpy()\n",
    "                        final_image = Image.fromarray(final_image.astype(np.uint8))\n",
    "                        image_number = sample_already_created + idx_img + idx_sub_batch * sub_batch_size\n",
    "                        final_image.save(save_folder+ f\"/image_{image_number}.png\")\n",
    "                    \n",
    "                        image_counter += 1\n",
    "\n",
    "                        if image_counter in whenFID:\n",
    "                            FIDs[f\"{sampler_type}_{image_counter}_{mapping_r[cls]}\"] = computeFidPytorch(\n",
    "                                                                                        truth_dataset=dataset_subdivided_dir + f\"/{mapping_r[cls]}\", \n",
    "                                                                                        generated_dataset=save_folder, \n",
    "                                                                                        device=\"cuda:2\"\n",
    "                                                                                        )\n",
    "                            with open(\"FIDs.npy\", \"wb\") as fout:\n",
    "                                np.save(fout, FIDs, allow_pickle=True)\n",
    "                        \n",
    "\n",
    "                    # Delete the sampling tensor to reuse GPU memory istantly\n",
    "                    del x_samples_ddim\n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated samples against the all dataset label by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:10<00:00, 247.59it/s] \n",
      "100%|██████████| 5889/5889 [00:00<00:00, 6567.36it/s]\n",
      "100%|██████████| 388/388 [00:00<00:00, 5871.90it/s]\n",
      "100%|██████████| 3531/3531 [00:00<00:00, 6348.82it/s]\n",
      "100%|██████████| 2493/2493 [00:00<00:00, 6338.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arancione\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:36<00:00,  8.15it/s]\n",
      "100%|██████████| 400/400 [02:28<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bianco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:36<00:00,  8.07it/s]\n",
      "100%|██████████| 200/200 [01:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:37<00:00,  8.00it/s]\n",
      "100%|██████████| 200/200 [00:58<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:37<00:00,  8.02it/s]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:37<00:00,  8.04it/s]\n",
      "100%|██████████| 200/200 [00:25<00:00,  7.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, tqdm\n",
    "\n",
    "dest_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_against\"\n",
    "\n",
    "labels = [\"arancione\", \"bianco\", \"blu\", \"nero\", \"verde\"]\n",
    "gpu_number = 0\n",
    "FIDs = {}\n",
    "with open(\"FIDs/fids_label_vs_allDataset.txt\", \"w\") as fout:\n",
    "    \n",
    "    # Make the folder with only the ground truth images for a specific label\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    src_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_subdivided_256\"\n",
    "    for label in labels:\n",
    "        for file in tqdm.tqdm(os.listdir(src_dir + f\"/{label}\")):\n",
    "                shutil.copy(src_dir + f\"/{label}/{file}\", dest_dir+f\"/{label}_{file}\")\n",
    "\n",
    "    for label in labels:\n",
    "        print(label)\n",
    "\n",
    "        FIDs[label] = computeFidPytorch(truth_dataset=dest_dir,\n",
    "                                    generated_dataset=f\"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/sample/ldm/wikiart/PLMS/75/{label}\",\n",
    "                                    device=f\"cuda:{gpu_number}\",)\n",
    "        fout.write(f\"{label} vs all dataset: {FIDs[label]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated samples against only its corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arancione\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2586/2586 [00:11<00:00, 224.25it/s] \n",
      "100%|██████████| 52/52 [00:07<00:00,  6.59it/s]\n",
      "100%|██████████| 400/400 [02:12<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bianco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5889/5889 [00:55<00:00, 106.52it/s]\n",
      "100%|██████████| 118/118 [00:15<00:00,  7.60it/s]\n",
      "100%|██████████| 200/200 [01:05<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 388/388 [00:01<00:00, 221.48it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.00it/s]\n",
      "100%|██████████| 200/200 [00:58<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3531/3531 [00:26<00:00, 131.70it/s]\n",
      "100%|██████████| 71/71 [00:10<00:00,  7.04it/s]\n",
      "100%|██████████| 200/200 [00:59<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2493/2493 [00:18<00:00, 134.83it/s]\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.49it/s]\n",
      "100%|██████████| 200/200 [00:55<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil, os, tqdm\n",
    "\n",
    "dest_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_against\"\n",
    "\n",
    "labels = [\"arancione\", \"bianco\", \"blu\", \"nero\", \"verde\"]\n",
    "gpu_number = 0\n",
    "FIDs = {}\n",
    "with open(\"/FIDs/fids_label_vs_label.txt\", \"w\") as fout:\n",
    "    for label in labels:\n",
    "        print(label)\n",
    "\n",
    "        # Make the folder with only the ground truth images for a specific label\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "            os.makedirs(dest_dir)\n",
    "            \n",
    "\n",
    "        src_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_subdivided_256\"\n",
    "\n",
    "        \n",
    "        for file in tqdm.tqdm(os.listdir(src_dir + f\"/{label}\")):\n",
    "            shutil.copy(src_dir + f\"/{label}/{file}\", dest_dir+f\"/{label}_{file}\")\n",
    "\n",
    "\n",
    "        FIDs[label] = computeFidPytorch(truth_dataset=dest_dir,\n",
    "                                    generated_dataset=f\"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/sample/ldm/wikiart/PLMS/75/{label}\",\n",
    "                                    device=f\"cuda:{gpu_number}\",)\n",
    "        fout.write(f\"{label} vs {label}: {FIDs[label]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'PLMS_10000_arancione': 100.54613,\n",
      "    'PLMS_10000_blu': 158.08682,\n",
      "    'PLMS_10000_verde': 102.91808,\n",
      "    'PLMS_1000_arancione': 126.48265,\n",
      "    'PLMS_1000_bianco': 110.8994,\n",
      "    'PLMS_1000_blu': 165.49686,\n",
      "    'PLMS_1000_verde': 126.81854,\n",
      "    'PLMS_11000_arancione': 100.53096,\n",
      "    'PLMS_12000_arancione': 100.344,\n",
      "    'PLMS_13000_arancione': 100.192444,\n",
      "    'PLMS_14000_arancione': 100.260376,\n",
      "    'PLMS_15000_arancione': 100.21661,\n",
      "    'PLMS_16000_arancione': 99.946884,\n",
      "    'PLMS_17000_arancione': 99.967636,\n",
      "    'PLMS_18000_arancione': 99.93763,\n",
      "    'PLMS_19000_arancione': 99.84171,\n",
      "    'PLMS_20000_arancione': 99.76372,\n",
      "    'PLMS_2000_arancione': 111.11384,\n",
      "    'PLMS_2000_blu': 162.07695,\n",
      "    'PLMS_2000_verde': 113.297745,\n",
      "    'PLMS_3000_arancione': 106.053406,\n",
      "    'PLMS_3000_blu': 160.27895,\n",
      "    'PLMS_3000_verde': 107.59354,\n",
      "    'PLMS_4000_arancione': 104.049576,\n",
      "    'PLMS_4000_blu': 159.28232,\n",
      "    'PLMS_4000_verde': 106.30329,\n",
      "    'PLMS_5000_arancione': 102.7775,\n",
      "    'PLMS_5000_blu': 158.65115,\n",
      "    'PLMS_5000_verde': 105.247536,\n",
      "    'PLMS_6000_arancione': 102.02391,\n",
      "    'PLMS_6000_blu': 158.69237,\n",
      "    'PLMS_6000_verde': 104.20015,\n",
      "    'PLMS_7000_arancione': 100.89682,\n",
      "    'PLMS_7000_blu': 158.35078,\n",
      "    'PLMS_7000_verde': 103.616875,\n",
      "    'PLMS_8000_arancione': 100.61143,\n",
      "    'PLMS_8000_blu': 158.24255,\n",
      "    'PLMS_8000_verde': 103.37252,\n",
      "    'PLMS_9000_arancione': 100.705956,\n",
      "    'PLMS_9000_blu': 158.26184,\n",
      "    'PLMS_9000_verde': 103.05628}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os\n",
    "\n",
    "FIDs = {}\n",
    "if os.path.exists(\"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/FIDs/FIDs.npy\"):\n",
    "    with open(\"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/FIDs/FIDs.npy\", \"rb\") as fin:\n",
    "        FIDs = np.load(fin, allow_pickle=True).item()\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(FIDs,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, tqdm, os\n",
    "labels = [\"arancione\",\"nero\", \"bianco\"]\n",
    "src_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_subdivided_256\"\n",
    "dest_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_against\"\n",
    "for label in labels:\n",
    "    for file in tqdm.tqdm(os.listdir(src_dir + f\"/{label}\")):\n",
    "        shutil.copy(src_dir + f\"/{label}/{file}\", dest_dir+f\"/{label}_{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, tqdm, os\n",
    "labels = [\"arancione\",\"nero\", \"bianco\"]\n",
    "src_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/sample/ldm/wikiart/PLMS/75\"\n",
    "dest_dir = \"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_generated_256\"\n",
    "for label in labels:\n",
    "    for file in tqdm.tqdm(os.listdir(src_dir + f\"/{label}\")[:9990]):\n",
    "        shutil.copy(src_dir + f\"/{label}/{file}\", dest_dir+f\"/{label}_{file}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylegan generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan3.git \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/styleGan_repos/sg3\"\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/styleGan_repos/sg2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, json\n",
    "\n",
    "checkPts_sg2_path = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/pretrained_model/StyleGans/absfig-sg2.pkl\"\n",
    "stylegan2outFolder = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/sample/styleGan2Out\"\n",
    "sg2_script_path = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/styleGan_repos/sg2/generate.py\"\n",
    "\n",
    "checkPts_sg3_path = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/pretrained_model/StyleGans/stylegan3-absfig-snapshot-000660.pkl\"\n",
    "stylegan3outFolder = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/sample/styleGan3Out\"\n",
    "sg3_script_path = \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/styleGan_repos/sg3/gen_images.py\"\n",
    "\n",
    "mapping_file = \"/mnt/data1/bardella_data/gitRepos/Thesis/dataset_preprocessing/wikiart/mapping.json\"\n",
    "\n",
    "n_classes = 8\n",
    "n_samples = 10000\n",
    "\n",
    "with open(mapping_file, \"r\") as fin:\n",
    "    mapping = json.load(fin)\n",
    "    mapping_r = {v:k for k, v in mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outFolder, checkPts, script_path in zip([stylegan2outFolder, stylegan3outFolder], [checkPts_sg2_path, checkPts_sg3_path], [sg2_script_path, sg3_script_path]):\n",
    "\n",
    "  for cls in range(n_classes):\n",
    "    outFolderfinal = outFolder + f\"/{mapping_r[cls]}\"\n",
    "    if not os.path.exists(outFolderfinal): \n",
    "      os.makedirs(outFolderfinal)\n",
    "\n",
    "    subprocess.run([\"python\", script_path,\n",
    "                      f\"--outdir={outFolderfinal}\",\n",
    "                      f\"--trunc={1}\",\n",
    "                      f\"--seeds=0-{n_samples}\",\n",
    "                      f\"--class={cls}\",\n",
    "                      f\"--network={checkPts}\",\n",
    "                      ], \n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID COMPUTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow inplementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this cell we need to have tensorflow Version 1.1 installed with python 3.7\n",
    "\n",
    "CLI command:\n",
    "\n",
    "conda install -c anaconda tensorflow-gpu=1.15 python=3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from pprint import pprint\n",
    "# import numpy\n",
    "\n",
    "# labels = [\"arancione\", \"bianco\", \"blu\", \"giallo\", \"nero\", \"rosso\", \"verde\", \"viola\"]\n",
    "# truth_label = \"arancione\"\n",
    "# generated_dataset = f\"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/sample/ldm/wikiart/200/{truth_label}\"\n",
    "\n",
    "# device =  \"0\"\n",
    "\n",
    "# FIDs = {}\n",
    "# for label in labels:\n",
    "\n",
    "#     truth_dataset = f\"/mnt/data1/bardella_data/gitRepos/Thesis/Datasets/wikiart_subdivided/{label}\"\n",
    "\n",
    "#     output = subprocess.run(['/home/bardella/.conda/envs/thesis_tf/bin/python', \n",
    "#                             \"/mnt/data1/bardella_data/gitRepos/Thesis/ldm_porting/modules/fid.py\", \n",
    "#                             truth_dataset,\n",
    "#                             generated_dataset,\n",
    "#                             \"--gpu\",\n",
    "#                             \"0\",\n",
    "#                             ], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "\n",
    "#     FIDs[\"_Vs_\".join((label, truth_label))] = numpy.float32(output.split(\" \")[-1][:-1])\n",
    "\n",
    "# pprint(FIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paths\n",
    "# import modules.fid as fid\n",
    "# import glob\n",
    "# from skimage import imread\n",
    "# import tensorflow as tf\n",
    "\n",
    "# image_path = '/tmp/images' # set path to some generated images\n",
    "# stats_path = 'fid_stats.npz' # training set statistics\n",
    "# inception_path = fid.check_or_download_inception(None) # download inception network\n",
    "\n",
    "# # loads all images into memory (this might require a lot of RAM!)\n",
    "# image_list = glob.glob(os.path.join(image_path, '*.jpg'))\n",
    "# images = np.array([imread(str(fn)).astype(np.float32) for fn in files])\n",
    "\n",
    "# # load precalculated training set statistics\n",
    "# f = np.load(stats_path)\n",
    "# mu_real, sigma_real = f['mu'][:], f['sigma'][:]\n",
    "# f.close()\n",
    "\n",
    "# fid.create_inception_graph(inception_path)  # load the graph into the current TF graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     mu_gen, sigma_gen = fid.calculate_activation_statistics(images, sess, batch_size=100)\n",
    "\n",
    "# fid_value = fid.calculate_frechet_distance(mu_gen, sigma_gen, mu_real, sigma_real)\n",
    "# print(\"FID: %s\" % fid_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
